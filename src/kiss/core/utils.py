# Author: Koushik Sen (ksen@berkeley.edu)
# Contributors:
# Koushik Sen (ksen@berkeley.edu)
# add your name here

"""Utility functions for the KISS core module."""

from pathlib import Path
from string import Formatter as StringFormatter
from typing import Any, TypeVar, cast

import yaml

from kiss.core.config import DEFAULT_CONFIG

T = TypeVar("T")

# Safari browser configuration for web scraping
SAFARI_USER_AGENT = (
    "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) "
    "AppleWebKit/605.1.15 (KHTML, like Gecko) "
    "Version/18.2 Safari/605.1.15"
)

SAFARI_HEADERS = {
    "User-Agent": SAFARI_USER_AGENT,
    "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8",
    "Accept-Language": "en-US,en;q=0.9",
}


def get_config_value(
    value: T | None, config_obj: Any, attr_name: str, default: T | None = None
) -> T:
    """Get a config value, preferring explicit value over config default.

    This eliminates the repetitive pattern:
        value if value is not None else config.attr_name

    Args:
        value: The explicitly provided value (may be None)
        config_obj: The config object to read from if value is None
        attr_name: The attribute name to read from config_obj
        default: Fallback default if both value and config attribute are None

    Returns:
        The resolved value (explicit value > config value > default)
    """
    if value is not None:
        return value
    config_value = getattr(config_obj, attr_name, None)
    if config_value is not None:
        return cast(T, config_value)
    if default is not None:
        return default
    raise ValueError(f"No value provided and config.{attr_name} is not set")


def get_template_field_names(text: str) -> list[str]:
    """Get the field names from the text.

    Args:
        text (str): The text containing template field placeholders.

    Returns:
        list[str]: A list of field names found in the text.
    """
    return [
        field_name
        for _, field_name, _, _ in StringFormatter().parse(text)
        if field_name is not None
    ]


def add_prefix_to_each_line(text: str, prefix: str) -> str:
    """Adds a prefix to each line of the text.

    Args:
        text (str): The text to add prefix to.
        prefix (str): The prefix to add to each line.

    Returns:
        str: The text with prefix added to each line.
    """
    return "\n".join([f"{prefix}{line}" for line in text.split("\n")])


def config_to_dict() -> dict[Any, Any]:
    """Convert the config to a dictionary.

    Returns:
        dict[Any, Any]: A dictionary representation of the default config.
    """

    def convert_to_json(obj: Any) -> Any:
        if isinstance(obj, dict):
            return {k: convert_to_json(v) for k, v in obj.items() if "API_KEY" not in k}
        if isinstance(obj, list):
            return [convert_to_json(item) for item in obj]
        if isinstance(obj, (str, int, float, bool, type(None))):
            return obj
        # For classes/objects, recursively convert fields, skipping any that end with API_KEY
        if hasattr(obj, "__dict__"):
            return {
                k: convert_to_json(getattr(obj, k))
                for k in obj.__dict__.keys()
                if "API_KEY" not in k
            }
        return obj

    result: dict[Any, Any] = convert_to_json(DEFAULT_CONFIG)
    return result


def fc(file_path: str) -> str:
    """Reads a file and returns the content.

    Args:
        file_path (str): The path to the file to read.

    Returns:
        str: The content of the file.
    """
    return Path(file_path).read_text()


def finish(
    status: str = "success",
    analysis: str = "",
    result: str = "",
) -> str:
    """
    The agent must call this function with the final status, analysis, and result
    when it has solved the given task. Status **MUST** be 'success' or 'failure'.

    Args:
        status: The status of the agent's task ('success' or 'failure'). Defaults to 'success'.
        analysis: The analysis of the agent's trajectory.
        result: The result generated by the agent.

    Returns:
        A YAML string containing the status, analysis, and result of the agent's task.
    """
    result_str = yaml.dump(
        {
            "status": status,
            "analysis": analysis,
            "result": result,
        },
        indent=2,
        sort_keys=False,
    )
    return cast(str, result_str)

def _fetch_page_content(url: str, headers: dict[str, str], max_content_length: int = 10000) -> str:
    """
    Fetch and extract text content from a URL using BeautifulSoup.

    Args:
        url: The URL to fetch.
        headers: HTTP headers to use for the request.
        max_content_length: Maximum length of content to return.

    Returns:
        Extracted text content from the page.
    """
    import re

    import requests
    from bs4 import BeautifulSoup

    try:
        response = requests.get(url, headers=headers, timeout=10, allow_redirects=True)
        response.raise_for_status()
        response.encoding = response.apparent_encoding or "utf-8"

        soup = BeautifulSoup(response.text, "html.parser")

        # Remove non-content elements
        non_content_tags = [
            "script", "style", "noscript", "header", "footer", "nav", "aside", "iframe", "svg"
        ]
        for tag in soup(non_content_tags):
            tag.decompose()

        # Find main content area
        main_content = (
            soup.find("main")
            or soup.find("article")
            or soup.find(attrs={"role": "main"})
            or soup.find(id=re.compile(r"content|main|article", re.IGNORECASE))
            or soup.find(class_=re.compile(r"content|main|article", re.IGNORECASE))
            or soup.body
            or soup
        )

        text = re.sub(r"\s+", " ", main_content.get_text(separator=" ", strip=True)).strip()

        if len(text) > max_content_length:
            text = text[:max_content_length] + "... [truncated]"

        return text or "No readable content found."
    except requests.exceptions.Timeout:
        return "Failed to fetch content: Request timed out."
    except requests.exceptions.HTTPError as e:
        return f"Failed to fetch content: HTTP {e.response.status_code}"
    except requests.exceptions.RequestException as e:
        return f"Failed to fetch content: {type(e).__name__}"
    except Exception as e:
        return f"Failed to fetch content: {str(e)}"


def _render_page_with_playwright(url: str, wait_selector: str | None = None) -> str:
    """
    Render a page using Playwright headless browser and return the HTML.

    Uses Safari/WebKit with anti-detection measures to avoid being blocked.

    Args:
        url: The URL to render.
        wait_selector: Optional CSS selector to wait for before extracting content.

    Returns:
        The rendered HTML content.
    """
    from playwright.sync_api import sync_playwright

    with sync_playwright() as p:
        browser = p.webkit.launch(headless=True)
        context = browser.new_context(
            user_agent=SAFARI_USER_AGENT,
            viewport={"width": 1920, "height": 1080},
            locale="en-US",
            timezone_id="America/Los_Angeles",
            java_script_enabled=True,
            has_touch=False,
            is_mobile=False,
            device_scale_factor=2,
            extra_http_headers={
                **SAFARI_HEADERS,
                "Accept-Encoding": "gzip, deflate, br",
                "Connection": "keep-alive",
                "Upgrade-Insecure-Requests": "1",
                "Sec-Fetch-Dest": "document",
                "Sec-Fetch-Mode": "navigate",
                "Sec-Fetch-Site": "none",
                "Sec-Fetch-User": "?1",
            },
        )
        page = context.new_page()

        try:
            page.goto(url, wait_until="networkidle", timeout=30000)
            if wait_selector:
                try:
                    page.wait_for_selector(wait_selector, timeout=10000)
                except Exception:
                    pass
            page.wait_for_timeout(2000)
            return page.content()
        finally:
            browser.close()


def _extract_search_results(soup: Any, selector: str, max_results: int) -> list[tuple[str, str]]:
    """Extract search result links from parsed HTML."""
    skip_domains = {"youtube.com", "maps.google", "accounts.google", "duckduckgo.com"}
    results: list[tuple[str, str]] = []

    for link in soup.select(selector):
        if len(results) >= max_results:
            break

        title = link.get_text(strip=True)
        href = link.get("href", "")
        url = href if isinstance(href, str) else (href[0] if href else "")

        if not title or not url or not url.startswith("http"):
            continue
        if any(domain in url for domain in skip_domains):
            continue

        results.append((title, url))

    return results


def search_web(query: str, max_results: int = 5) -> str:
    """
    Perform a web search and return the top search results with page contents.

    Tries DuckDuckGo first (more reliable for automated access), then falls back
    to Startpage if needed. Uses Playwright headless browser with Safari/WebKit
    to render JavaScript and avoid bot detection.

    Args:
        query: The search query.
        max_results: Maximum number of results to fetch content for. Defaults to 5.

    Returns:
        A string containing titles, links, and page contents of the top search results.
    """
    from urllib.parse import quote_plus

    from bs4 import BeautifulSoup

    # Search providers to try in order
    providers = [
        (
            f"https://duckduckgo.com/?q={quote_plus(query)}&t=h_&ia=web",
            "a[data-testid='result-title-a']",
        ),
        (
            f"https://www.startpage.com/sp/search?query={quote_plus(query)}",
            "a.result-link",
        ),
    ]

    for url, selector in providers:
        try:
            html = _render_page_with_playwright(url, wait_selector=selector)
            if "captcha" in html.lower():
                continue

            soup = BeautifulSoup(html, "html.parser")
            results = _extract_search_results(soup, selector, max_results)

            if results:
                formatted_results = []
                for title, result_url in results:
                    content = _fetch_page_content(result_url, SAFARI_HEADERS)
                    result_text = f"Title: {title}\nURL: {result_url}\nContent:\n{content}\n"
                    formatted_results.append(result_text)

                ret = "\n---\n".join(formatted_results)
                print("Web search results:", ret)
                return ret
        except Exception:
            continue

    return "No search results found."

